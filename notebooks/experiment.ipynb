{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Semantic Search Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\David\\semantic_search\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "import pandas as pd\n",
        "import torch\n",
        "from src.pipeline import SemanticSearchPipeline\n",
        "\n",
        "print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“¦ Found 2 cached embeddings (1.38 GB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing (basic): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57734/57734 [00:02<00:00, 24612.72it/s]\n",
            "c:\\Users\\David\\semantic_search\\notebooks\\..\\src\\embeddings.py:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n",
            "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 151/151 [00:26<00:00,  5.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline MRR: 0.3529\n"
          ]
        }
      ],
      "source": [
        "# Baseline test\n",
        "baseline = SemanticSearchPipeline({\n",
        "    'embedding_model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
        "    'index_type': 'flat',\n",
        "    'use_reranker': False,\n",
        "    'text_strategy': 'basic'\n",
        "})\n",
        "\n",
        "# Quick test on 10% sample\n",
        "metrics = baseline.run_full_pipeline('small', sample_frac=0.1)\n",
        "print(f'Baseline MRR: {metrics[\"MRR\"]:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“¦ Found 3 cached embeddings (1.46 GB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing (basic): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57734/57734 [00:02<00:00, 20768.42it/s]\n",
            "c:\\Users\\David\\semantic_search\\notebooks\\..\\src\\embeddings.py:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With Reranker MRR: 0.4282\n"
          ]
        }
      ],
      "source": [
        "# Test with reranker\n",
        "with_reranker = SemanticSearchPipeline({\n",
        "    'embedding_model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
        "    'index_type': 'ivf',\n",
        "    'use_reranker': True,\n",
        "    'text_strategy': 'basic'\n",
        "})\n",
        "\n",
        "metrics = with_reranker.run_full_pipeline('small', sample_frac=0.1)\n",
        "print(f'With Reranker MRR: {metrics[\"MRR\"]:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“¦ Found 3 cached embeddings (1.46 GB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing (enhanced): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57734/57734 [00:03<00:00, 16093.93it/s]\n",
            "c:\\Users\\David\\semantic_search\\notebooks\\..\\src\\embeddings.py:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n",
            "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 151/151 [00:41<00:00,  3.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced Text MRR: 0.4352\n"
          ]
        }
      ],
      "source": [
        "# Test enhanced text preparation\n",
        "enhanced = SemanticSearchPipeline({\n",
        "    'embedding_model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
        "    'index_type': 'ivf',\n",
        "    'use_reranker': True,\n",
        "    'text_strategy': 'enhanced'\n",
        "})\n",
        "\n",
        "metrics = enhanced.run_full_pipeline('small', sample_frac=0.1)\n",
        "print(f'Enhanced Text MRR: {metrics[\"MRR\"]:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "OPTIMIZATION EXPERIMENTS\n",
            "Baseline MRR: 0.4887\n",
            "======================================================================\n",
            "\n",
            "1. Testing basic text + reranker...\n",
            "ðŸ“¦ Found 4 cached embeddings (1.54 GB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing (basic): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57734/57734 [00:02<00:00, 23850.42it/s]\n",
            "c:\\Users\\David\\semantic_search\\notebooks\\..\\src\\embeddings.py:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: MRR = 0.4282\n",
            "\n",
            "2. Testing enhanced text + reranker...\n",
            "ðŸ“¦ Found 4 cached embeddings (1.54 GB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing (enhanced): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57734/57734 [00:04<00:00, 12266.17it/s]\n",
            "c:\\Users\\David\\semantic_search\\notebooks\\..\\src\\embeddings.py:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        }
      ],
      "source": [
        "# Run optimization experiments\n",
        "from src.experiments import run_optimization_experiments\n",
        "\n",
        "results = run_optimization_experiments(sample_frac=0.1)\n",
        "\n",
        "# Display results\n",
        "df = pd.DataFrame(results)\n",
        "df[['name', 'Hits@1', 'Hits@5', 'Hits@10', 'MRR']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
